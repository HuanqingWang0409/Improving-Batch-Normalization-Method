{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W6wm6cdNQKKB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "6514e892ef414c55a8795b0560d54ba3",
            "aef787e98b504ac3820ce0f108b7d11b",
            "df7efe96c569497abd1a34c83977a760",
            "c4e3ffbe9ead42a9b15eb2e1dc4b881b",
            "ff1cd881d90d4753b489733d1eb60d82",
            "2235630df8ce47a6938711ae79137483",
            "4a71b67826e1407e9db2ac497517d2ed",
            "7cb737b71adf4787b00e62c80070de17",
            "b90232839edf447d8a3d043dc0f6a4d8",
            "7aa8d6e7a4024ccb838cc9e56fa33e8a",
            "e953bfbc5d7045e29f41f09967f950b7"
          ]
        },
        "outputId": "e00661ae-2b74-4284-d91a-6e27a7c7ad9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6514e892ef414c55a8795b0560d54ba3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch, math, copy\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils.np_utils import to_categorical   \n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "#(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#train_dataset = datasets.MNIST(\"data\", train=True, download=True, transform=transform)\n",
        "train_dataset = datasets.CIFAR10(\"data\", train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "#test_dataset = datasets.MNIST(\"data\", train=False, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(\"data\", train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "\n",
        "def train(epochs, model, criterion, optimizer, train_loader, test_loader):\n",
        "    for epoch in range(epochs):\n",
        "        train_err = train_epoch(model, criterion, optimizer, train_loader)\n",
        "        test_err = test(model, test_loader)\n",
        "        print('Epoch {:03d}/{:03d}, Train Error {:.2f}% || Test Error {:.2f}%'.format(epoch, epochs, train_err*100, test_err*100))\n",
        "    return train_err, test_err\n",
        "    \n",
        "def train_epoch(model, criterion, optimizer, loader):\n",
        "    total_correct = 0.\n",
        "    total_samples = 0.\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        # insert code to feed the data to the model and collect its output\n",
        "        output = model(data)\n",
        "\n",
        "        # insert code to compute the loss from output and the true target\n",
        "        loss = criterion(output,target)\n",
        "\n",
        "        # insert code to update total_correct and total_samples\n",
        "        # total_correct: total number of correctly classified samples\n",
        "        # total_samples: total number of samples seen so far\n",
        "        total_samples += len(target)\n",
        "        pred = output.argmax(dim=1, keepdim=True)  \n",
        "        total_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            \n",
        "        # insert code to update the parameters using optimizer\n",
        "        # be careful in this part as an incorrect implementation will affect\n",
        "        # all your experiments and have a significant impact on your grade!\n",
        "        # in particular, note that pytorch does --not-- automatically\n",
        "        # clear the parameter's gradients: check tutorials to see\n",
        "        # how this can be done with a single method call.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    return 1 - total_correct/total_samples\n",
        "    \n",
        "def test(model, loader):\n",
        "    total_correct = 0.\n",
        "    total_samples = 0.\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(loader):\n",
        "            if torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # insert code to feed the data to the model and collect its output\n",
        "            output = model(data)\n",
        "\n",
        "            # insert code to update total_correct and total_samples\n",
        "            # total_correct: total number of correctly classified samples\n",
        "            # total_samples: total number of samples seen so far\n",
        "            total_samples += len(target)\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            total_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            \n",
        "\n",
        "\n",
        "    return 1 - total_correct/total_samples\n",
        "\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCJMS5bcRJql",
        "outputId": "14a93be5-fd75-4c61-96df-542ceddaba07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l==0.17.0 in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.0) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.0) (2.23.0)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.0) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.0) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.0) (1.1.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (7.6.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (5.6.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.17.0) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.17.0) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.17.0) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.17.0) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.17.0) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.17.0) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.17.0) (1.0.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.17.0) (3.5.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (4.9.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (4.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (0.18.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (5.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (3.7.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.17.0) (0.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.17.0) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.17.0) (1.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==0.17.0) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==0.17.0) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.17.0) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l==0.17.0) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.17.0) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.17.0) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.17.0) (0.11.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (0.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (4.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==0.17.0) (21.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==0.17.0) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==0.17.0) (2018.9)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l==0.17.0) (2.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.17.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.17.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.17.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.17.0) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l==0.17.0\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "\n",
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    # Use `is_grad_enabled` to determine whether the current mode is training\n",
        "    # mode or prediction mode\n",
        "    if not torch.is_grad_enabled():\n",
        "        # If it is prediction mode, directly use the mean and variance\n",
        "        # obtained by moving average\n",
        "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "        #if torch.mean(moving_var)>torch.mean(gamma):\n",
        "        Y = (torch.sqrt(moving_var)+9.*gamma)/10. * X_hat + (9.*beta+moving_mean)/10.\n",
        "        #else:\n",
        "        #  Y=X_hat * torch.sqrt(moving_var + eps) + beta\n",
        "    \n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # When using a fully-connected layer, calculate the mean and\n",
        "            # variance on the feature dimension\n",
        "            mean = X.mean(dim=0)\n",
        "            var = ((X - mean) ** 2).mean(dim=0)\n",
        "            X_hat = (X - mean) / torch.sqrt(var)\n",
        "            \n",
        "        else:\n",
        "            # When using a two-dimensional convolutional layer, calculate the\n",
        "            # mean and variance on the channel dimension (axis=1). Here we\n",
        "            # need to maintain the shape of `X`, so that the broadcasting\n",
        "            # operation can be carried out later\n",
        "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
        "        # In training mode, the current mean and variance are used for the\n",
        "        # standardization\n",
        "            X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "            \n",
        "            \n",
        "        #if torch.mean(var)>torch.mean(gamma):\n",
        "        Y = (9.*gamma+torch.sqrt(var))/10. * X_hat + (9.*beta+mean)/10.  # Scale and shift\n",
        "        #else:\n",
        "        #  Y=X_hat *torch.sqrt(var) + beta      \n",
        "    \n",
        "        # Update the mean and variance using moving average\n",
        "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
        "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
        "        \n",
        "    return Y, moving_mean.data, moving_var.data\n",
        "\n",
        "\n",
        "class BatchNorm(nn.Module):\n",
        "    # `num_features`: the number of outputs for a fully-connected layer\n",
        "    # or the number of output channels for a convolutional layer. `num_dims`:\n",
        "    # 2 for a fully-connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0 and 1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If `X` is not on the main memory, copy `moving_mean` and\n",
        "        # `moving_var` to the device where `X` is located\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated `moving_mean` and `moving_var`\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.9)\n",
        "        return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-tgwCvIfSqUB"
      },
      "outputs": [],
      "source": [
        "#Define a new class to connect the residual with ELU, BatchNorm, and Conv.\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv=nn.Sequential(nn.Conv2d(in_channels, in_channels, 3, 1, 1),\n",
        "                                 BatchNorm(in_channels, num_dims=4),\n",
        "                                 nn.ELU())\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # insert code to compute sigma\n",
        "                sigma = np.sqrt(1/(m.out_channels*m.kernel_size[0]*m.kernel_size[0]))\n",
        "                m.weight.data.normal_(0, sigma)\n",
        "                m.bias.data.zero_()\n",
        "        \n",
        "    def forward(self, input):        \n",
        "        u=self.conv(input)+input\n",
        "        return u\n",
        "        \n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        # write code here to instantiate layers\n",
        "        # for example, self.conv = nn.Conv2d(1, 4, 3, 1, 1)\n",
        "        # creates a conv layer with 1 input channel, 4 output\n",
        "        # channels, a 3x3 kernel, and stride=padding=1\n",
        "        self.conv1=[]\n",
        "        self.conv1.append(nn.Conv2d(1, 4, 3, 1, 1))\n",
        "        self.conv1.append(BatchNorm(4, num_dims=4))\n",
        "        self.conv1.append(nn.ELU())\n",
        "        for i in range((k//3)-1):\n",
        "         self.conv1.append(ResBlock(4))         \n",
        "                \n",
        "        self.conv2=[]\n",
        "        self.conv2.append(nn.Conv2d(4, 8, 3, 1, 1))\n",
        "        self.conv2.append(BatchNorm(8, num_dims=4))\n",
        "        self.conv2.append(nn.ELU())\n",
        "        for i in range((k//3)-1):\n",
        "         self.conv2.append(ResBlock(8))\n",
        "                 \n",
        "        self.conv3=[]\n",
        "        self.conv3.append(nn.Conv2d(8, 16, 3, 1, 1))\n",
        "        self.conv3.append(BatchNorm(16, num_dims=4))\n",
        "        self.conv3.append(nn.ELU())\n",
        "        for i in range((k//3)-1):\n",
        "         self.conv3.append(ResBlock(16))         \n",
        "        \n",
        "        self.conv1=nn.Sequential(*self.conv1)\n",
        "        self.conv2=nn.Sequential(*self.conv2)\n",
        "        self.conv3=nn.Sequential(*self.conv3)       \n",
        "       \n",
        "        self.ln = nn.Linear(144, 10)\n",
        "        self.flat = nn.Flatten()     \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # insert code to compute sigma\n",
        "                sigma = np.sqrt(1/(m.out_channels*m.kernel_size[0]*m.kernel_size[0]))\n",
        "                m.weight.data.normal_(0, sigma)\n",
        "                m.bias.data.zero_()\n",
        "        \n",
        "    def forward(self, input):\n",
        "        \n",
        "        # write code here to define how the output u is computed\n",
        "        # from the input and the model's layers\n",
        "        # for example, u = self.conv(input) defines u\n",
        "        # to be simply the output of self.conv given 'input'\n",
        "        x=self.conv1(input) \n",
        "        x=F.avg_pool2d(x,kernel_size=2,stride=2)\n",
        "        \n",
        "        x=self.conv2(x) \n",
        "        x=F.avg_pool2d(x,kernel_size=2,stride=2)\n",
        "        \n",
        "        x=self.conv3(x) \n",
        "        x=F.avg_pool2d(x,kernel_size=2,stride=2)\n",
        "              \n",
        "        u=self.flat(x)\n",
        "        u=self.ln(u) \n",
        "        return u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXX2xdsXRG2m",
        "outputId": "e73f6ec3-b3c7-457f-8e83-a293f8f82ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training ResNet with 24 layers\n",
            "Epoch 000/003, Train Error 59.95% || Test Error 52.69%\n",
            "Epoch 001/003, Train Error 58.23% || Test Error 50.05%\n",
            "Epoch 002/003, Train Error 47.53% || Test Error 45.27%\n",
            "\n",
            "Training ResNet with 36 layers\n",
            "Epoch 000/003, Train Error 62.41% || Test Error 53.92%\n",
            "Epoch 001/003, Train Error 54.01% || Test Error 49.53%\n",
            "Epoch 002/003, Train Error 46.99% || Test Error 45.50%\n"
          ]
        }
      ],
      "source": [
        "#Convex combination improvement:\n",
        "lr = 0.01\n",
        "for k in [24, 36]:\n",
        "  print(\"\\nTraining ResNet with {} layers\".format(k))\n",
        "  model = ResNet(k).cuda()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "  train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8WHHC9uCe0ql"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Original BN paper\n",
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    # Use `is_grad_enabled` to determine whether the current mode is training\n",
        "    # mode or prediction mode\n",
        "    if not torch.is_grad_enabled():\n",
        "        # If it is prediction mode, directly use the mean and variance\n",
        "        # obtained by moving average\n",
        "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # When using a fully-connected layer, calculate the mean and\n",
        "            # variance on the feature dimension\n",
        "            mean = X.mean(dim=0)\n",
        "            var = ((X - mean) ** 2).mean(dim=0)\n",
        "        else:\n",
        "            # When using a two-dimensional convolutional layer, calculate the\n",
        "            # mean and variance on the channel dimension (axis=1). Here we\n",
        "            # need to maintain the shape of `X`, so that the broadcasting\n",
        "            # operation can be carried out later\n",
        "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
        "        # In training mode, the current mean and variance are used for the\n",
        "        # standardization\n",
        "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "        # Update the mean and variance using moving average\n",
        "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
        "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_mean.data, moving_var.data\n",
        "\n",
        "class BatchNorm(nn.Module):\n",
        "    # `num_features`: the number of outputs for a fully-connected layer\n",
        "    # or the number of output channels for a convolutional layer. `num_dims`:\n",
        "    # 2 for a fully-connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0 and 1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If `X` is not on the main memory, copy `moving_mean` and\n",
        "        # `moving_var` to the device where `X` is located\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated `moving_mean` and `moving_var`\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.9)\n",
        "        return Y\n",
        "\n",
        "\n",
        "lr = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSPm-xBfbvEc",
        "outputId": "b9abf4e0-2d10-456f-9be6-b59131e31c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training ResNet with 60 layers\n",
            "Epoch 000/003, Train Error 71.73% || Test Error 61.80%\n",
            "Epoch 001/003, Train Error 59.02% || Test Error 56.25%\n",
            "Epoch 002/003, Train Error 53.16% || Test Error 51.02%\n"
          ]
        }
      ],
      "source": [
        "k=60\n",
        "print(\"\\nTraining ResNet with {} layers\".format(k))\n",
        "model = ResNet(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BSRn3i-vTj8-"
      },
      "outputs": [],
      "source": [
        "#Change intitilaization\n",
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    # Use `is_grad_enabled` to determine whether the current mode is training\n",
        "    # mode or prediction mode\n",
        "    if not torch.is_grad_enabled():\n",
        "        # If it is prediction mode, directly use the mean and variance\n",
        "        # obtained by moving average\n",
        "        X_hat = (X - moving_mean) #/ torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # When using a fully-connected layer, calculate the mean and\n",
        "            # variance on the feature dimension\n",
        "            mean = X.mean(dim=0)\n",
        "            var = ((X - mean) ** 2).mean(dim=0)\n",
        "        else:\n",
        "            # When using a two-dimensional convolutional layer, calculate the\n",
        "            # mean and variance on the channel dimension (axis=1). Here we\n",
        "            # need to maintain the shape of `X`, so that the broadcasting\n",
        "            # operation can be carried out later\n",
        "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
        "        # In training mode, the current mean and variance are used for the\n",
        "        # standardization\n",
        "        X_hat = (X - mean) #/ torch.sqrt(var + eps)\n",
        "        # Update the mean and variance using moving average\n",
        "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
        "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_mean.data, moving_var.data\n",
        "\n",
        "\n",
        "class BatchNorm(nn.Module):\n",
        "    # `num_features`: the number of outputs for a fully-connected layer\n",
        "    # or the number of output channels for a convolutional layer. `num_dims`:\n",
        "    # 2 for a fully-connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.init=True\n",
        "        self.gamma = nn.Parameter(torch.ones(shape)) \n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0 and 1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If `X` is not on the main memory, copy `moving_mean` and\n",
        "        # `moving_var` to the device where `X` is located\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated `moving_mean` and `moving_var`\n",
        "        if self.init:\n",
        "          self.init=False\n",
        "          mean = X.mean(dim=0)\n",
        "          var = ((X - mean) ** 2).mean(dim=0)            \n",
        "          for i in self.gamma:\n",
        "            i = i/torch.sqrt(var)\n",
        "\n",
        "\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.9)\n",
        "        return Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C43jNjPabDwT",
        "outputId": "44069da8-787e-4821-c2e5-1d0b70b98ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training ResNet with 3 layers\n",
            "Epoch 000/003, Train Error 63.13% || Test Error 56.59%\n",
            "Epoch 001/003, Train Error 53.28% || Test Error 50.80%\n",
            "Epoch 002/003, Train Error 48.83% || Test Error 48.20%\n",
            "\n",
            "Training ResNet with 6 layers\n",
            "Epoch 000/003, Train Error 60.92% || Test Error 53.04%\n",
            "Epoch 001/003, Train Error 53.17% || Test Error 48.54%\n",
            "Epoch 002/003, Train Error 45.94% || Test Error 45.63%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.01\n",
        "for k in[3, 6]:\n",
        "  print(\"\\nTraining ResNet with {} layers\".format(k))\n",
        "  model = ResNet(k).cuda()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "  train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZP8hGMBo_73",
        "outputId": "9ca3de4e-0e2b-425d-a552-85a668064320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training ResNet with 12 layers\n",
            "Epoch 000/003, Train Error 60.86% || Test Error 53.57%\n",
            "Epoch 001/003, Train Error 54.22% || Test Error 50.24%\n",
            "Epoch 002/003, Train Error 47.39% || Test Error 45.22%\n"
          ]
        }
      ],
      "source": [
        "#Not using\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv=nn.Sequential(nn.Conv2d(in_channels, in_channels, 3, 1, 1),\n",
        "                                 nn.BatchNorm2d(in_channels),\n",
        "                                 nn.ELU())\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # insert code to compute sigma\n",
        "                sigma = np.sqrt(1/(m.out_channels*m.kernel_size[0]*m.kernel_size[0]))\n",
        "                m.weight.data.normal_(0, sigma)\n",
        "                m.bias.data.zero_()\n",
        "        \n",
        "    def forward(self, input):        \n",
        "        u=self.conv(input)+input\n",
        "        return u\n",
        "        \n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        # write code here to instantiate layers\n",
        "        # for example, self.conv = nn.Conv2d(1, 4, 3, 1, 1)\n",
        "        # creates a conv layer with 1 input channel, 4 output\n",
        "        # channels, a 3x3 kernel, and stride=padding=1\n",
        "        self.conv1=[]\n",
        "        self.conv1.append(nn.Conv2d(3, 3, 3, 1, 1))\n",
        "        self.conv1.append(nn.BatchNorm2d(3))\n",
        "        self.conv1.append(nn.ELU())\n",
        "        for i in range((k//3)-1):\n",
        "         self.conv1.append(ResBlock(3))         \n",
        "                \n",
        "        self.conv2=[]\n",
        "        self.conv2.append(nn.Conv2d(3, 6, 3, 1, 1))\n",
        "        self.conv2.append(nn.BatchNorm2d(6))\n",
        "        self.conv2.append(nn.ELU())\n",
        "        for i in range((k//3)-1):\n",
        "         self.conv2.append(ResBlock(6))\n",
        "                 \n",
        "        self.conv3=[]\n",
        "        self.conv3.append(nn.Conv2d(6, 12, 3, 1, 1))\n",
        "        self.conv3.append(nn.BatchNorm2d(12))\n",
        "        self.conv3.append(nn.ELU())\n",
        "        for i in range((k//3)-1):\n",
        "         self.conv3.append(ResBlock(12))         \n",
        "        \n",
        "        self.conv1=nn.Sequential(*self.conv1)\n",
        "        self.conv2=nn.Sequential(*self.conv2)\n",
        "        self.conv3=nn.Sequential(*self.conv3)       \n",
        "       \n",
        "        self.ln = nn.Linear(192, 10)\n",
        "        self.flat = nn.Flatten()     \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # insert code to compute sigma\n",
        "                sigma = np.sqrt(1/(m.out_channels*m.kernel_size[0]*m.kernel_size[0]))\n",
        "                m.weight.data.normal_(0, sigma)\n",
        "                m.bias.data.zero_()\n",
        "        \n",
        "    def forward(self, input):\n",
        "        \n",
        "        # write code here to define how the output u is computed\n",
        "        # from the input and the model's layers\n",
        "        # for example, u = self.conv(input) defines u\n",
        "        # to be simply the output of self.conv given 'input'\n",
        "        x=self.conv1(input) \n",
        "        x=F.avg_pool2d(x,kernel_size=2,stride=2)\n",
        "        \n",
        "        x=self.conv2(x) \n",
        "        x=F.avg_pool2d(x,kernel_size=2,stride=2)\n",
        "        \n",
        "        x=self.conv3(x) \n",
        "        x=F.avg_pool2d(x,kernel_size=2,stride=2)\n",
        "              \n",
        "        u=self.flat(x)\n",
        "        u=self.ln(u) \n",
        "        return u\n",
        "\n",
        "   \n",
        "k = 12\n",
        "lr = 0.01\n",
        "\n",
        "print(\"\\nTraining ResNet with {} layers\".format(k))\n",
        "model = ResNet(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6514e892ef414c55a8795b0560d54ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aef787e98b504ac3820ce0f108b7d11b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_df7efe96c569497abd1a34c83977a760",
              "IPY_MODEL_c4e3ffbe9ead42a9b15eb2e1dc4b881b",
              "IPY_MODEL_ff1cd881d90d4753b489733d1eb60d82"
            ]
          }
        },
        "aef787e98b504ac3820ce0f108b7d11b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df7efe96c569497abd1a34c83977a760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2235630df8ce47a6938711ae79137483",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a71b67826e1407e9db2ac497517d2ed"
          }
        },
        "c4e3ffbe9ead42a9b15eb2e1dc4b881b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7cb737b71adf4787b00e62c80070de17",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b90232839edf447d8a3d043dc0f6a4d8"
          }
        },
        "ff1cd881d90d4753b489733d1eb60d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7aa8d6e7a4024ccb838cc9e56fa33e8a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:05&lt;00:00, 33567143.89it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e953bfbc5d7045e29f41f09967f950b7"
          }
        },
        "2235630df8ce47a6938711ae79137483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a71b67826e1407e9db2ac497517d2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7cb737b71adf4787b00e62c80070de17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b90232839edf447d8a3d043dc0f6a4d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7aa8d6e7a4024ccb838cc9e56fa33e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e953bfbc5d7045e29f41f09967f950b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}